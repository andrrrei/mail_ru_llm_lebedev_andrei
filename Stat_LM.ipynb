{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10bf3438",
      "metadata": {
        "id": "10bf3438"
      },
      "source": [
        "# Ассистент 1 - LM на основе n-грамм"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de848df",
      "metadata": {
        "id": "6de848df"
      },
      "source": [
        "Перед вами - первое дополнительное задание повышенной сложности, в рамках которого вам предстоит начать разработку телеграм-бота с генеративными моделями.\n",
        "\n",
        "Цель данного ноутбука - помочь влиться в разработку ассистента. В данном ноутбуке написан код для \"обучения\" LM на основе n-грамм, для генерации с помощью нее текста, а также сохранение и загрузка модели и токенизатора.\n",
        "\n",
        "Относитесь к данному заданию максимально творчески - любую часть кода можно менять под ваши нужды и желания, можно оптимизировать, добавлять методы генерации, использовать любые данные, обучать сколь угодно \"большую\" модель.\n",
        "\n",
        "При этом вам стоит быть готовыми со всеми техническими проблеми справляться самому - именно так обычно происходит в реальной жизни в реальных проектах :)\n",
        "\n",
        "Поэтому отдельно подчеркну:\n",
        "* если что-то сломалось после ваших изменений - подразумевается, что вы сами найдете проблему и исправите\n",
        "* если вы ничего не трогали, но что-то не работает у нас - подразумевается, что вы сами найдете проблему и исправите :)\n",
        "\n",
        "Главный критерий выполнености данного задания - телеграм-бот, генерирующий текст и использующий обозначенный в задании подход (в случае данного ноутбука - n-граммная модель в любой ее реализации)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31ca60e8",
      "metadata": {
        "id": "31ca60e8"
      },
      "source": [
        "Для обучения качественной модели вам потребуются датасеты. В ноутбуке составлен маленький игрушечный датасет, вам для улучшения качества потребуется данные в большем количестве и более качественные, а также другие параметры модели и генерации (например, размер контекста побольше).\n",
        "\n",
        "С нормальным датасетом и правильными параметрами даже такой простой моделью можно добиться адекватного качества генерации текста (возможно не очень человечный, но вполне связный текст).\n",
        "\n",
        "Датасеты можно найти и выбрать тут (желательно на русском, вам так будет понятней качество и в целом полезней):\n",
        "https://huggingface.co/datasets\n",
        "  \n",
        "Можете найти наиболее интересный для себя датасет (можете сделать модель как смешной, так и полезной), либо выбрать любой из этих датасетов\n",
        "* https://huggingface.co/datasets/Den4ikAI/russian_dialogues\n",
        "* https://huggingface.co/datasets/Georgii/russianPoetry\n",
        "* https://huggingface.co/datasets/IgorVolochay/russian_jokes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ab7f83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27ab7f83",
        "outputId": "948ee9ae-3eaa-4ca9-bfbc-b329d6e150ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c6c33a9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T14:50:17.414585Z",
          "iopub.status.busy": "2024-02-21T14:50:17.413838Z",
          "iopub.status.idle": "2024-02-21T14:50:37.187697Z",
          "shell.execute_reply": "2024-02-21T14:50:37.186316Z",
          "shell.execute_reply.started": "2024-02-21T14:50:17.414553Z"
        },
        "id": "0c6c33a9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "from itertools import chain\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "from typing import List, Dict, Optional, Iterable, Tuple\n",
        "\n",
        "#from tqdm.notebook import tqdm\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.processors import TemplateProcessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df809daa",
      "metadata": {
        "id": "df809daa"
      },
      "source": [
        "Токенизатор разбивает текст на слова. Можно попробовать другие способы токенизации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "id": "90160389",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:11.789756Z",
          "iopub.status.busy": "2024-02-21T15:03:11.788899Z",
          "iopub.status.idle": "2024-02-21T15:03:11.808584Z",
          "shell.execute_reply": "2024-02-21T15:03:11.807519Z",
          "shell.execute_reply.started": "2024-02-21T15:03:11.789725Z"
        },
        "id": "90160389"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self,\n",
        "                 token_pattern: str = '\\w+|[\\!\\?\\,\\.\\-\\:]',\n",
        "                 eos_token: str = '<EOS>',\n",
        "                 pad_token: str = '<PAD>',\n",
        "                 unk_token: str = '<UNK>'):\n",
        "        self.token_pattern = token_pattern\n",
        "        self.eos_token = eos_token\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "\n",
        "        self.special_tokens = [self.eos_token, self.pad_token, self.unk_token]\n",
        "        self.vocab = None\n",
        "        self.inverse_vocab = None\n",
        "\n",
        "    def text_preprocess(self, input_text: str) -> str:\n",
        "        \"\"\" Предобрабатываем один текст \"\"\"\n",
        "        input_text = input_text.lower()               # приведение к нижнему регистру\n",
        "        input_text = re.sub('\\s+', ' ', input_text)   # унифицируем пробелы\n",
        "        input_text = input_text.strip()               # удаляем пробелы по бокам\n",
        "        return input_text\n",
        "\n",
        "    def _tokenize(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
        "        text = self.text_preprocess(text)\n",
        "        tokens = re.findall(self.token_pattern, text)\n",
        "        if append_eos_token:\n",
        "            tokens.append(self.eos_token)\n",
        "        return tokens\n",
        "\n",
        "    def build_vocab(self, corpus: List[str]) -> None:\n",
        "        assert len(corpus)\n",
        "        all_tokens = defaultdict(int)\n",
        "        for text in corpus:\n",
        "            tokens = list(self._tokenize(text, append_eos_token=False))\n",
        "            for i in tokens:\n",
        "                all_tokens[i] += 1\n",
        "        all_tokens = {word: cnt for word, cnt in all_tokens.items() if cnt >= 10}\n",
        "        all_tokens = list(all_tokens.keys())\n",
        "        self.vocab = {elem: ind for ind, elem in enumerate(all_tokens)}\n",
        "        special_tokens = [self.eos_token, self.unk_token, self.pad_token]\n",
        "        for token in special_tokens:\n",
        "            self.vocab[token] = len(self.vocab)\n",
        "        self.inverse_vocab = {ind: elem for elem, ind in self.vocab.items()}\n",
        "        return self\n",
        "\n",
        "    def encode(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
        "        \"\"\" Токенизируем текст \"\"\"\n",
        "        tokens = self._tokenize(text, append_eos_token)\n",
        "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, input_ids: Iterable[int], remove_special_tokens: bool = False) -> str:\n",
        "        assert len(input_ids)\n",
        "        assert max(input_ids) < len(self.vocab) and min(input_ids) >= 0\n",
        "        tokens = []\n",
        "        for ind in input_ids:\n",
        "            token = self.inverse_vocab[ind]\n",
        "            if remove_special_tokens and token in self.special_tokens:\n",
        "                continue\n",
        "            tokens.append(token)\n",
        "        text = ' '.join( tokens )\n",
        "        return text\n",
        "\n",
        "    def save(self, path: str) -> bool:\n",
        "        data = {\n",
        "            'token_pattern': self.token_pattern,\n",
        "            'eos_token': self.eos_token,\n",
        "            'pad_token': self.pad_token,\n",
        "            'unk_token': self.unk_token,\n",
        "            'special_tokens': self.special_tokens,\n",
        "            'vocab': self.vocab,\n",
        "            'inverse_vocab': self.inverse_vocab,\n",
        "        }\n",
        "\n",
        "        with open(path, 'wb') as fout:\n",
        "            pickle.dump(data, fout)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def load(self, path: str) -> bool:\n",
        "        with open(path, 'rb') as fin:\n",
        "            data = pickle.load(fin)\n",
        "\n",
        "        self.token_pattern = data['token_pattern']\n",
        "        self.eos_token = data['eos_token']\n",
        "        self.pad_token = data['pad_token']\n",
        "        self.unk_token = data['unk_token']\n",
        "        self.special_tokens = data['special_tokens']\n",
        "        self.vocab = data['vocab']\n",
        "        self.inverse_vocab = data['inverse_vocab']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6999f620",
      "metadata": {
        "id": "6999f620"
      },
      "source": [
        "Класс для задания параметров генерации, так удобней писать логику для валидации параметров и разные другие доп методы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "id": "f14d758b",
      "metadata": {
        "id": "f14d758b"
      },
      "outputs": [],
      "source": [
        "class GenerationConfig:\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Тут можно задать любые параметры и их значения по умолчанию\n",
        "        Значения для стратегии декодирования decoding_strategy: ['max', 'top-p', 'top-k]\n",
        "        \"\"\"\n",
        "        self.temperature = kwargs.pop(\"temperature\", 0.5)\n",
        "        self.max_tokens = kwargs.pop(\"max_tokens\", 8)\n",
        "        self.sample_top_p = kwargs.pop(\"sample_top_p\", 0.7)\n",
        "        self.sample_top_k = kwargs.pop(\"sample_top_k\", 10)\n",
        "        self.decoding_strategy = kwargs.pop(\"decoding_strategy\", 'top-p')\n",
        "        self.remove_special_tokens = kwargs.pop(\"remove_special_tokens\", False)\n",
        "        self.validate()\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\" Здесь можно валидировать параметры \"\"\"\n",
        "        if not (1.0 > self.sample_top_p > 0):\n",
        "            raise ValueError('sample_top_p')\n",
        "        if self.decoding_strategy not in ['max', 'top-p', 'top-k']:\n",
        "            raise ValueError('decoding_strategy')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd923a4",
      "metadata": {
        "id": "6bd923a4"
      },
      "source": [
        "Сама LM на основе n-грамм. Тут используется сглаживание Лапласа (можно поменять на метод backoff при желании), а также есть ряд параметров, сильно влияющий на качество генерации. Один из параметров генерации - стратегия генерации.\n",
        "\n",
        "Когда мы получили вероятности для следующего токена, мы по этим вероятностям хотим выбрать этот следующий токен.\n",
        "\n",
        "Можно просто семплировать из этого распределения - но тогда есть шанс, что будут очень маловероятные токены.\n",
        "\n",
        "Можно брать самый вероятный токен - но это плохо повлияет на разнообразие и \"человечность\" языка\n",
        "\n",
        "Можно воспользовать подходом top-p - семплировать только из тех токенов, которые наиболее вероятны (их вероятности суммируются в заданный p)\n",
        "\n",
        "Можно проверить, что top-p будет генерировать более интересный текст чем max\n",
        "\n",
        "Также обратите внимание на параметр температуры. В случае top-p и семплирования, чем больше делаешь температуру, тем меньше отличаются друг от друга вероятности (распределение стремится к равномерному, даже если исходное распределение имело вполне себе выраженные максимумы), и текст становится более случайным (и разнообразным)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "id": "462316de",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:21.268282Z",
          "iopub.status.busy": "2024-02-21T15:03:21.267567Z",
          "iopub.status.idle": "2024-02-21T15:03:21.292501Z",
          "shell.execute_reply": "2024-02-21T15:03:21.291323Z",
          "shell.execute_reply.started": "2024-02-21T15:03:21.268251Z"
        },
        "id": "462316de"
      },
      "outputs": [],
      "source": [
        "class StatLM:\n",
        "    def __init__(self,\n",
        "                 tokenizer: Tokenizer,\n",
        "                 context_size: int = 3,\n",
        "                 alpha: float = 0.1\n",
        "                ):\n",
        "\n",
        "        assert context_size >= 2\n",
        "\n",
        "        self.context_size = context_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.n_gramms_stat = defaultdict(int)\n",
        "        self.nx_gramms_stat = defaultdict(int)\n",
        "\n",
        "    # def get_token_by_ind(ind: int) -> str:\n",
        "    #     return self.tokenizer.vocab.get(ind)\n",
        "\n",
        "    # def get_ind_by_token(token: str) -> int:\n",
        "    #     return self.tokenizer.inverse_vocab.get(token, self.tokenizer.inverse_vocab[self.unk_token])\n",
        "\n",
        "    def train(self, train_texts: List[str]):\n",
        "        for sentence in tqdm.tqdm(train_texts, desc='train lines'):\n",
        "            sentence_ind = self.tokenizer.encode(sentence)\n",
        "            for i in range(len(sentence_ind) - self.context_size):\n",
        "\n",
        "                seq = tuple(sentence_ind[i: i + self.context_size - 1])\n",
        "                self.n_gramms_stat[seq] += 1\n",
        "\n",
        "                seq_x = tuple(sentence_ind[i: i + self.context_size])\n",
        "                self.nx_gramms_stat[seq_x] += 1\n",
        "\n",
        "            seq = tuple(sentence_ind[len(sentence_ind) - self.context_size:])\n",
        "            self.n_gramms_stat[seq] += 1\n",
        "\n",
        "    def sample_token(self,\n",
        "                     token_distribution: np.ndarray,\n",
        "                     generation_config: GenerationConfig) -> int:\n",
        "        if generation_config.decoding_strategy == 'max':\n",
        "            return token_distribution.argmax()\n",
        "        elif generation_config.decoding_strategy == 'top-p':\n",
        "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))),\n",
        "                                        reverse=True)\n",
        "            total_proba = 0.0\n",
        "            tokens_to_sample = []\n",
        "            tokens_probas = []\n",
        "            for token_proba, ind in token_distribution:\n",
        "                tokens_to_sample.append(ind)\n",
        "                tokens_probas.append(token_proba)\n",
        "                total_proba += token_proba\n",
        "                if total_proba >= generation_config.sample_top_p:\n",
        "                    break\n",
        "            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n",
        "            tokens_probas = np.array(tokens_probas) / generation_config.temperature\n",
        "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
        "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
        "        elif generation_config.decoding_strategy == 'top-k':\n",
        "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))),\n",
        "                                        reverse=True)[:generation_config.sample_top_k]\n",
        "            tokens_to_sample = []\n",
        "            tokens_probas = []\n",
        "            for token_proba, ind in token_distribution:\n",
        "                tokens_to_sample.append(ind)\n",
        "                tokens_probas.append(token_proba)\n",
        "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
        "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
        "        else:\n",
        "            raise ValueError(f'Unknown decoding strategy: {generation_config.decoding_strategy}')\n",
        "\n",
        "    def save_stat(self, path: str) -> bool:\n",
        "        stat = {\n",
        "            'n_gramms_stat': self.n_gramms_stat,\n",
        "            'nx_gramms_stat': self.nx_gramms_stat,\n",
        "            'context_size': self.context_size,\n",
        "            'alpha': self.alpha\n",
        "        }\n",
        "        with open(path, 'wb') as fout:\n",
        "            pickle.dump(stat, fout)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def load_stat(self, path: str) -> bool:\n",
        "        with open(path, 'rb') as fin:\n",
        "            stat = pickle.load(fin)\n",
        "\n",
        "        self.n_gramms_stat = stat['n_gramms_stat']\n",
        "        self.nx_gramms_stat = stat['nx_gramms_stat']\n",
        "        self.context_size = stat['context_size']\n",
        "        self.alpha = stat['alpha']\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_stat(self) -> Dict[str, Dict]:\n",
        "\n",
        "        n_token_stat, nx_token_stat = {}, {}\n",
        "        for token_inds, count in self.n_gramms_stat.items():\n",
        "            n_token_stat[self.tokenizer.decode(token_inds)] = count\n",
        "\n",
        "        for token_inds, count in self.nx_gramms_stat.items():\n",
        "            nx_token_stat[self.tokenizer.decode(token_inds)] = count\n",
        "\n",
        "        return {\n",
        "            'n gramms stat': self.n_gramms_stat,\n",
        "            'n+1 gramms stat': self.nx_gramms_stat,\n",
        "            'n tokens stat': n_token_stat,\n",
        "            'n+1 tokens stat': nx_token_stat,\n",
        "        }\n",
        "\n",
        "    def _get_next_token(self,\n",
        "                        tokens: List[int],\n",
        "                        generation_config: GenerationConfig) -> (int, str):\n",
        "        denominator = self.n_gramms_stat.get(tuple(tokens), 0) + self.alpha * len(self.tokenizer.vocab)\n",
        "        numerators = []\n",
        "        for ind in self.tokenizer.inverse_vocab:\n",
        "            numerators.append(self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) + self.alpha)\n",
        "\n",
        "        token_distribution = np.array(numerators) / denominator\n",
        "        max_proba_ind = self.sample_token(token_distribution, generation_config)\n",
        "\n",
        "        next_token = self.tokenizer.inverse_vocab[max_proba_ind]\n",
        "\n",
        "        return max_proba_ind, next_token\n",
        "\n",
        "    def generate_token(self,\n",
        "                       text: str,\n",
        "                       generation_config: GenerationConfig\n",
        "                      ) -> Dict:\n",
        "        tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
        "        tokens = tokens[-self.context_size + 1:]\n",
        "\n",
        "        max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
        "\n",
        "        return {\n",
        "            'next_token': next_token,\n",
        "            'next_token_num': max_proba_ind,\n",
        "        }\n",
        "\n",
        "\n",
        "    def generate_text(self, text: str,\n",
        "                      generation_config: GenerationConfig\n",
        "                     ) -> Dict:\n",
        "\n",
        "        all_tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
        "        tokens = all_tokens[-self.context_size + 1:]\n",
        "\n",
        "        next_token = None\n",
        "        while next_token != self.tokenizer.eos_token and len(all_tokens) < generation_config.max_tokens:\n",
        "            max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
        "            all_tokens.append(max_proba_ind)\n",
        "            tokens = all_tokens[-self.context_size + 1:]\n",
        "\n",
        "        new_text = self.tokenizer.decode(all_tokens, generation_config.remove_special_tokens)\n",
        "\n",
        "        finish_reason = 'max tokens'\n",
        "        if all_tokens[-1] == self.tokenizer.vocab[self.tokenizer.eos_token]:\n",
        "            finish_reason = 'end of text'\n",
        "\n",
        "        return {\n",
        "            'all_tokens': all_tokens,\n",
        "            'total_text': new_text,\n",
        "            'finish_reason': finish_reason\n",
        "        }\n",
        "\n",
        "    def generate(self, text: str, generation_config: Dict) -> str:\n",
        "        return self.generate_text(text, generation_config)['total_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f60f19",
      "metadata": {
        "id": "90f60f19"
      },
      "source": [
        "Эта функция напрямую используется в телеграм боте для получения модели и конфига генерации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "id": "4cfdad05",
      "metadata": {
        "id": "4cfdad05"
      },
      "outputs": [],
      "source": [
        "def construct_model():\n",
        "    config = {\n",
        "        'temperature': 0.5,\n",
        "        'max_tokens': 8,\n",
        "        'sample_top_p': 0.9,\n",
        "        'sample_top_k': 10,\n",
        "        'decoding_strategy': 'top-p',\n",
        "    }\n",
        "\n",
        "    stat_lm_path = 'models/stat_lm/stat_lm.pkl'\n",
        "    tokenizer_path = 'models/stat_lm/tokenizer.pkl'\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.load(tokenizer_path)\n",
        "\n",
        "    stat_lm = StatLM(tokenizer)\n",
        "    stat_lm.load_stat(stat_lm_path)\n",
        "\n",
        "    generation_config = GenerationConfig(temperature=config['temperature'],\n",
        "                                         max_tokens=config['max_tokens'],\n",
        "                                         sample_top_p=config['sample_top_p'],\n",
        "                                         sample_top_k=config['sample_top_k'],\n",
        "                                         decoding_strategy=config['decoding_strategy'],\n",
        "                                         remove_special_tokens=True)\n",
        "\n",
        "    kwargs = {'generation_config': generation_config}\n",
        "    return stat_lm, kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a85b767",
      "metadata": {
        "id": "3a85b767"
      },
      "source": [
        "### Обучение на датасетах IgorVolochay/russian_jokes и SiberiaSoft/SiberianPersonaChat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oatnyZe9Q_K9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oatnyZe9Q_K9",
        "outputId": "2b8d7a83-fe46-4e9a-8818-eca9d40c391f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset1 = load_dataset('SiberiaSoft/SiberianPersonaChat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2115c30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2115c30",
        "outputId": "f5ba0283-b4cf-41db-dc24-d6235ac8aa50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['output', 'name', 'input'],\n",
              "        num_rows: 448506\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3baddff",
      "metadata": {
        "id": "d3baddff"
      },
      "outputs": [],
      "source": [
        "dataset1 = dataset1['train']['output']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5hBEbUQWWURb",
      "metadata": {
        "id": "5hBEbUQWWURb"
      },
      "outputs": [],
      "source": [
        "dataset2 = load_dataset('IgorVolochay/russian_jokes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M6jcK7g0Weaq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6jcK7g0Weaq",
        "outputId": "b3420cb8-9c5a-4e44-e94f-0621c6cd832f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 150553\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hO0DPGDrWj8I",
      "metadata": {
        "id": "hO0DPGDrWj8I"
      },
      "outputs": [],
      "source": [
        "dataset2 = dataset2['train']['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "id": "F6XBAco6YEv9",
      "metadata": {
        "id": "F6XBAco6YEv9"
      },
      "outputs": [],
      "source": [
        "dataset3 = load_dataset('Den4ikAI/russian_dialogues')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "id": "AzYGOqdYYJ2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzYGOqdYYJ2x",
        "outputId": "bcd7fc23-a2fe-45f2-d36b-86c86c5f3bbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'answer', 'relevance'],\n",
              "        num_rows: 2477321\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 227,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "id": "jHZxow2QYNrR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "beae8446eea245ba9769f21c617245cc",
            "8445239782744a628f99921145c058f1",
            "a4449c5e9aaa4e59a23a8a1082f3ddac",
            "adc1fb8dbebb4786829d5d2988d11ba6",
            "5000e48d24824f68ba4e9769fbf2b253",
            "817baf17c74f46d8a5c2696e93036966",
            "bdc04e4e56834996942425827f86b9c7",
            "6bc61d8241c94d9eba2cc4082503357d",
            "579856d7fbfd47c3aba3d3f83277b3b5",
            "e5461183f1fe4b938b1c3a560702b1e2",
            "2a8a2e78f5894a0eb913824918b170b3"
          ]
        },
        "id": "jHZxow2QYNrR",
        "outputId": "e46dee92-a5c8-4a6e-d6e1-d602c5543142"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "beae8446eea245ba9769f21c617245cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/2477321 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset3 = dataset3['train']\n",
        "dataset3 = dataset3.filter(lambda i: i['relevance'] == 1 and i['question'] != None and i['answer'] != None)\n",
        "dataset3 = dataset3.remove_columns(['relevance'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "id": "EV2xUuLnYmJX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV2xUuLnYmJX",
        "outputId": "8db5f1b6-08ad-481a-fdc2-ceeda2408900"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 43199/43199 [00:59<00:00, 725.91it/s]\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(dataset3, batch_size = 32)\n",
        "data_iter = tqdm.tqdm(\n",
        "    enumerate(train_loader),\n",
        "    total=len(train_loader),\n",
        "    bar_format=\"{l_bar}{r_bar}\"\n",
        ")\n",
        "\n",
        "texts = []\n",
        "for i, data in data_iter:\n",
        "    texts.append(data)\n",
        "\n",
        "dataset3 = []\n",
        "for i in range(len(texts)):\n",
        "    for j in range(len(texts[i]['question'])):\n",
        "        dataset3.append(texts[i]['question'][j])\n",
        "        dataset3.append(texts[i]['answer'][j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "id": "hkZSaAvAWvr3",
      "metadata": {
        "id": "hkZSaAvAWvr3"
      },
      "outputs": [],
      "source": [
        "dataset = dataset1 + dataset2 + dataset3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "id": "98e92a50",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:25.218697Z",
          "iopub.status.busy": "2024-02-21T15:03:25.218225Z",
          "iopub.status.idle": "2024-02-21T15:03:25.223271Z",
          "shell.execute_reply": "2024-02-21T15:03:25.222261Z",
          "shell.execute_reply.started": "2024-02-21T15:03:25.218665Z"
        },
        "id": "98e92a50"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer().build_vocab(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "id": "bxb75M0f458k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxb75M0f458k",
        "outputId": "bd4517ce-336d-4588-99d9-212b32b06dd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "138129"
            ]
          },
          "execution_count": 233,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dict(list(tokenizer.vocab.items())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef0948d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:29.936948Z",
          "iopub.status.busy": "2024-02-21T15:03:29.936563Z",
          "iopub.status.idle": "2024-02-21T15:03:29.957657Z",
          "shell.execute_reply": "2024-02-21T15:03:29.956641Z",
          "shell.execute_reply.started": "2024-02-21T15:03:29.936919Z"
        },
        "id": "2ef0948d",
        "outputId": "00583006-c085-4ee7-e6fe-51f34e99472d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train lines:  96%|█████████▋| 3239183/3363789 [04:36<00:09, 12944.22it/s]"
          ]
        }
      ],
      "source": [
        "stat_lm = StatLM(tokenizer, context_size=3, alpha=0.1)\n",
        "\n",
        "stat_lm.train(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-1JfUG_Xdr8Q",
      "metadata": {
        "id": "-1JfUG_Xdr8Q"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(temperature=0.0001, max_tokens=10,\n",
        "                                     sample_top_p=0.001, sample_top_k=10,\n",
        "                                     decoding_strategy='top-p', remove_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0h8PS3M_kjgV",
      "metadata": {
        "id": "0h8PS3M_kjgV"
      },
      "outputs": [],
      "source": [
        "# print(stat_lm.generate(\"привет\", generation_config))\n",
        "# print(stat_lm.generate(\"как тебя\", generation_config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d054a1d4",
      "metadata": {
        "id": "d054a1d4"
      },
      "outputs": [],
      "source": [
        "tokenizer.save('models/stat_lm/tokenizer.pkl')\n",
        "stat_lm.save_stat('models/stat_lm/stat_lm.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "600bcef7",
      "metadata": {
        "id": "600bcef7"
      },
      "source": [
        "Тут мы для токенизатора сохраняем только спецтокены и словарь, для модели - параметры и статистики n-грамм и n+1-грамм. Потом в телеграм боте подгружаем именно эти параметры"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e659ff6",
      "metadata": {
        "id": "0e659ff6"
      },
      "source": [
        "### Конструируем"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad87959",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0ad87959",
        "outputId": "8a7db079-33dd-4752-fd4c-9c596974ae62"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'вот такие пироги ! приседаний неотложных mindfulness лютую'"
            ]
          },
          "execution_count": 179,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model, kwargs = construct_model()\n",
        "\n",
        "model.generate(\"вот такие пироги!\", **kwargs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2a8a2e78f5894a0eb913824918b170b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5000e48d24824f68ba4e9769fbf2b253": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "579856d7fbfd47c3aba3d3f83277b3b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bc61d8241c94d9eba2cc4082503357d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817baf17c74f46d8a5c2696e93036966": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8445239782744a628f99921145c058f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_817baf17c74f46d8a5c2696e93036966",
            "placeholder": "​",
            "style": "IPY_MODEL_bdc04e4e56834996942425827f86b9c7",
            "value": "Filter: 100%"
          }
        },
        "a4449c5e9aaa4e59a23a8a1082f3ddac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bc61d8241c94d9eba2cc4082503357d",
            "max": 2477321,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_579856d7fbfd47c3aba3d3f83277b3b5",
            "value": 2477321
          }
        },
        "adc1fb8dbebb4786829d5d2988d11ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5461183f1fe4b938b1c3a560702b1e2",
            "placeholder": "​",
            "style": "IPY_MODEL_2a8a2e78f5894a0eb913824918b170b3",
            "value": " 2477321/2477321 [00:25&lt;00:00, 114059.30 examples/s]"
          }
        },
        "bdc04e4e56834996942425827f86b9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beae8446eea245ba9769f21c617245cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8445239782744a628f99921145c058f1",
              "IPY_MODEL_a4449c5e9aaa4e59a23a8a1082f3ddac",
              "IPY_MODEL_adc1fb8dbebb4786829d5d2988d11ba6"
            ],
            "layout": "IPY_MODEL_5000e48d24824f68ba4e9769fbf2b253"
          }
        },
        "e5461183f1fe4b938b1c3a560702b1e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
